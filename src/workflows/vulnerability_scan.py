"""
Vulnerability scanning workflow - Automated vulnerability detection.

This workflow:
1. Gets new/changed assets from diff engine
2. Scans with appropriate Nuclei templates
3. Uses Interactsh for OOB detection
4. Deduplicates findings
5. Stores results in database
6. Sends alerts for critical/high findings
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set

from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner

from src.core.diff_engine import DiffEngine
from src.db.models import AlertType, SeverityLevel, Vulnerability
from src.db.repositories import AssetRepository, ProgramRepository, VulnerabilityRepository
from src.db.session import db_manager
from src.scanners.interactsh import InteractshClient
from src.scanners.nuclei import NucleiScanner
from src.utils.logging import get_logger

logger = get_logger(__name__)


@task(name="identify_scan_targets", retries=1)
async def identify_scan_targets_task(
    program_id: int,
    force_rescan: bool = False,
) -> Dict[str, List[str]]:
    """
    Identify which assets need vulnerability scanning.
    
    Uses diff engine logic:
    - New assets â†’ Full scan
    - Modified assets â†’ Targeted scan
    - Unchanged assets â†’ Skip (unless force_rescan)
    """
    async with db_manager.get_session() as session:
        asset_repo = AssetRepository(session)
        diff_engine = DiffEngine(session)
        
        # Get all alive assets
        assets = await asset_repo.get_by_program(program_id, alive_only=True)
        
        new_targets = []
        modified_targets = []
        changed_fields_map: Dict[str, Set[str]] = {}
        
        for asset in assets:
            # Check if should scan
            should_scan, reason = await diff_engine.should_scan(asset, force=force_rescan)
            
            if not should_scan:
                continue
            
            # Build URL (prefer https)
            url = f"https://{asset.value}"
            
            # Categorize based on age and changes
            if "New asset" in reason or "first scan" in reason.lower():
                new_targets.append(url)
            elif "modified" in reason.lower():
                modified_targets.append(url)
                # Track what changed for this asset
                # In production, you'd query asset_changes table
                changed_fields_map[url] = set()  # TODO: Get from asset_changes
        
        logger.info(f"ðŸŽ¯ Scan targets identified:")
        logger.info(f"   New assets: {len(new_targets)}")
        logger.info(f"   Modified assets: {len(modified_targets)}")
        
        return {
            "new": new_targets,
            "modified": modified_targets,
            "changed_fields": changed_fields_map,
        }


@task(name="scan_with_nuclei", retries=1, retry_delay_seconds=60)
async def scan_with_nuclei_task(
    targets: List[str],
    scan_type: str = "full",
    changed_fields: Optional[Set[str]] = None,
    interactsh_domain: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Scan targets with Nuclei.
    
    Args:
        targets: List of URLs to scan
        scan_type: 'new' (full scan) or 'modified' (targeted)
        changed_fields: For modified assets, what changed
        interactsh_domain: Interactsh domain for OOB detection
    """
    if not targets:
        return []
    
    scanner = NucleiScanner()
    
    if scan_type == "new":
        logger.info(f"ðŸ†• Full scan of {len(targets)} new assets")
        return await scanner.scan_new_assets_only(targets, full_templates=True)
    elif scan_type == "modified":
        logger.info(f"ðŸ”„ Targeted scan of {len(targets)} modified assets")
        return await scanner.scan_changed_assets(targets, changed_fields or set())
    else:
        # Generic scan
        return await scanner.scan(
            targets,
            severity=["critical", "high", "medium"],
            interactsh_url=f"https://{interactsh_domain}" if interactsh_domain else None,
        )


@task(name="setup_interactsh")
async def setup_interactsh_task(server_url: str) -> Optional[str]:
    """Setup Interactsh for OOB detection."""
    try:
        client = InteractshClient(server_url=server_url)
        domain = await client.register()
        logger.info(f"ðŸŒ Interactsh ready: {domain}")
        # Note: In production, store client in state for later polling
        return domain
    except Exception as e:
        logger.warning(f"Interactsh setup failed (continuing without OOB): {e}")
        return None


@task(name="poll_interactsh")
async def poll_interactsh_task(
    domain: str,
    server_url: str,
    timeout: int = 120,
) -> List[Dict[str, Any]]:
    """Poll Interactsh for OOB interactions."""
    if not domain:
        return []
    
    try:
        client = InteractshClient(server_url=server_url)
        client.domain = domain
        interactions = await client.poll_interactions(timeout=timeout)
        await client.close()
        
        if interactions:
            logger.info(f"ðŸŽ¯ Detected {len(interactions)} OOB interactions!")
        
        return interactions
    except Exception as e:
        logger.error(f"Interactsh polling failed: {e}")
        return []


@task(name="deduplicate_findings")
async def deduplicate_findings_task(
    program_id: int,
    findings: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    Deduplicate vulnerability findings.
    
    Check if we've already seen this vulnerability on this asset.
    """
    if not findings:
        return []
    
    async with db_manager.get_session() as session:
        vuln_repo = VulnerabilityRepository(session)
        asset_repo = AssetRepository(session)
        
        # Get all assets for this program
        assets = await asset_repo.get_by_program(program_id)
        asset_map = {f"https://{a.value}": a for a in assets}
        
        # Get existing vulnerabilities (last 30 days to avoid huge queries)
        # TODO: Add method to get recent vulns by program
        
        new_findings = []
        for finding in findings:
            matched_at = finding.get("matched_at", "")
            template_id = finding.get("template_id", "")
            
            # Simple dedup: Check if same template_id + matched_at exists
            # In production, implement more sophisticated logic in VulnerabilityRepository
            
            # For now, consider all findings as new
            # TODO: Implement proper deduplication query
            new_findings.append(finding)
        
        logger.info(f"Deduplicated: {len(findings)} â†’ {len(new_findings)} new findings")
        return new_findings


@task(name="save_vulnerabilities")
async def save_vulnerabilities_task(
    program_id: int,
    findings: List[Dict[str, Any]],
) -> int:
    """Save vulnerability findings to database."""
    if not findings:
        return 0
    
    saved_count = 0
    
    async with db_manager.get_session() as session:
        vuln_repo = VulnerabilityRepository(session)
        asset_repo = AssetRepository(session)
        
        # Get assets map
        assets = await asset_repo.get_by_program(program_id)
        asset_map = {}
        for asset in assets:
            for prefix in ["https://", "http://"]:
                asset_map[f"{prefix}{asset.value}"] = asset
        
        for finding in findings:
            try:
                matched_at = finding.get("matched_at", "")
                
                # Find corresponding asset
                asset = None
                for url_prefix, a in asset_map.items():
                    if matched_at.startswith(url_prefix):
                        asset = a
                        break
                
                if not asset:
                    logger.warning(f"No asset found for: {matched_at}")
                    continue
                
                # Map severity
                severity_str = finding.get("severity", "info").upper()
                try:
                    severity = SeverityLevel[severity_str]
                except KeyError:
                    severity = SeverityLevel.INFO
                
                # Create vulnerability
                await vuln_repo.create(
                    asset_id=asset.id,
                    template_id=finding.get("template_id", ""),
                    name=finding.get("name", ""),
                    severity=severity,
                    matched_at=matched_at,
                    matcher_name=finding.get("matcher_name"),
                    extracted_results=finding.get("extracted_results"),
                    request=finding.get("request"),
                    response=finding.get("response"),
                    curl_command=finding.get("curl_command"),
                    tags=finding.get("tags"),
                    reference=finding.get("reference"),
                )
                
                saved_count += 1
                
            except Exception as e:
                logger.error(f"Failed to save vulnerability: {e}")
        
        await session.commit()
        logger.info(f"ðŸ’¾ Saved {saved_count} vulnerabilities to database")
    
    return saved_count


@task(name="send_vulnerability_alerts", retries=2, retry_delay_seconds=30)
async def send_vulnerability_alerts_task(
    program_id: int,
    findings: List[Dict[str, Any]],
) -> None:
    """
    Send alerts for new vulnerability findings.
    
    Separates critical findings for immediate individual alerts,
    batches other findings based on configuration.
    """
    if not findings:
        return
    
    from src.config import settings
    
    # Check if alerts are configured
    if not settings.discord_webhook_url and not settings.slack_webhook_url:
        logger.info("â© Skipping alerts (no channels configured)")
        return
    
    async with db_manager.get_session() as session:
        from src.alerting.manager import AlertManager
        from src.db.models import Vulnerability
        
        # Load vulnerabilities from database (need full objects with relationships)
        vuln_repo = VulnerabilityRepository(session)
        
        # Map findings to vulnerability IDs
        # For simplicity, we'll query the most recent vulnerabilities for this program
        recent_vulns = await vuln_repo.get_by_program(
            program_id, 
            is_new=True, 
            limit=len(findings) + 10  # Get a few extra in case of timing
        )
        
        if not recent_vulns:
            logger.warning("No recent vulnerabilities found for alerting")
            return
        
        # Separate critical/high from others
        critical_vulns = []
        other_vulns = []
        
        for vuln in recent_vulns[:len(findings)]:  # Only process new ones
            if vuln.severity in [SeverityLevel.CRITICAL, SeverityLevel.HIGH]:
                critical_vulns.append(vuln)
            else:
                other_vulns.append(vuln)
        
        async with AlertManager(session) as manager:
            # Send individual alerts for critical/high findings
            if critical_vulns:
                logger.info(f"ðŸš¨ Sending {len(critical_vulns)} critical/high severity alerts")
                for vuln in critical_vulns:
                    alert_type = (
                        AlertType.CRITICAL_FINDING 
                        if vuln.severity == SeverityLevel.CRITICAL 
                        else AlertType.NEW_VULNERABILITY
                    )
                    await manager.alert_vulnerability(vuln, alert_type)
            
            # Batch other findings
            if other_vulns:
                logger.info(f"ðŸ“¦ Sending batch alert for {len(other_vulns)} other findings")
                await manager.alert_batch_vulnerabilities(
                    other_vulns,
                    title=f"New Vulnerabilities Detected ({len(other_vulns)} findings)"
                )
        
        logger.info(f"âœ… Alerts sent successfully")


@flow(
    name="vulnerability_scan_flow",
    description="Automated vulnerability scanning with Nuclei",
    task_runner=ConcurrentTaskRunner(),
)
async def vulnerability_scan_flow(
    program_id: int,
    force_rescan: bool = False,
    enable_interactsh: bool = True,
    interactsh_server: str = "https://interact.sh",
) -> Dict[str, Any]:
    """
    Main vulnerability scanning workflow.
    
    Args:
        program_id: Database ID of program to scan
        force_rescan: Force rescan of all assets (ignore diff engine)
        enable_interactsh: Enable OOB detection with Interactsh
        interactsh_server: Interactsh server URL
        
    Returns:
        Dict with scan statistics
    """
    logger.info("=" * 70)
    logger.info(f"ðŸ” VULNERABILITY SCAN - Program ID: {program_id}")
    logger.info("=" * 70)
    
    start_time = datetime.utcnow()
    
    # Step 1: Setup Interactsh (if enabled)
    interactsh_domain = None
    if enable_interactsh:
        logger.info("ðŸŒ Phase 1: Setting up Interactsh for OOB detection")
        interactsh_domain = await setup_interactsh_task(interactsh_server)
    
    # Step 2: Identify scan targets
    logger.info("ðŸŽ¯ Phase 2: Identifying scan targets (Diff Engine)")
    targets = await identify_scan_targets_task(program_id, force_rescan)
    
    new_targets = targets["new"]
    modified_targets = targets["modified"]
    
    if not new_targets and not modified_targets:
        logger.info("âœ… No assets need scanning (all up to date)")
        return {"status": "no_scan_needed", "reason": "All assets up to date"}
    
    all_findings = []
    
    # Step 3: Scan new assets (full scan)
    if new_targets:
        logger.info(f"ðŸ†• Phase 3a: Scanning {len(new_targets)} NEW assets")
        new_findings = await scan_with_nuclei_task(
            new_targets,
            scan_type="new",
            interactsh_domain=interactsh_domain,
        )
        all_findings.extend(new_findings)
        logger.info(f"   Found {len(new_findings)} vulnerabilities in new assets")
    
    # Step 4: Scan modified assets (targeted scan)
    if modified_targets:
        logger.info(f"ðŸ”„ Phase 3b: Scanning {len(modified_targets)} MODIFIED assets")
        modified_findings = await scan_with_nuclei_task(
            modified_targets,
            scan_type="modified",
            changed_fields=set(),  # TODO: Pass actual changed fields
            interactsh_domain=interactsh_domain,
        )
        all_findings.extend(modified_findings)
        logger.info(f"   Found {len(modified_findings)} vulnerabilities in modified assets")
    
    # Step 5: Poll Interactsh for OOB interactions
    oob_interactions = []
    if enable_interactsh and interactsh_domain:
        logger.info("ðŸŒ Phase 4: Polling Interactsh for OOB interactions")
        oob_interactions = await poll_interactsh_task(
            interactsh_domain,
            interactsh_server,
            timeout=60,
        )
    
    # Step 6: Deduplicate findings
    logger.info("ðŸ” Phase 5: Deduplicating findings")
    unique_findings = await deduplicate_findings_task(program_id, all_findings)
    
    # Step 7: Save to database
    logger.info("ðŸ’¾ Phase 6: Saving vulnerabilities to database")
    saved_count = await save_vulnerabilities_task(program_id, unique_findings)
    
    # Calculate statistics
    duration = (datetime.utcnow() - start_time).total_seconds()
    
    # Group by severity
    severity_counts = {
        "critical": 0,
        "high": 0,
        "medium": 0,
        "low": 0,
        "info": 0,
    }
    
    for finding in unique_findings:
        severity = finding.get("severity", "info")
        if severity in severity_counts:
            severity_counts[severity] += 1
    
    # Final report
    report = {
        "program_id": program_id,
        "duration_seconds": duration,
        "targets_scanned": len(new_targets) + len(modified_targets),
        "new_assets_scanned": len(new_targets),
        "modified_assets_scanned": len(modified_targets),
        "total_findings": len(all_findings),
        "unique_findings": len(unique_findings),
        "saved_to_db": saved_count,
        "oob_interactions": len(oob_interactions),
        "severity_breakdown": severity_counts,
        "timestamp": datetime.utcnow().isoformat(),
    }
    
    logger.info("=" * 70)
    logger.info("ðŸ“Š VULNERABILITY SCAN COMPLETE!")
    logger.info("=" * 70)
    logger.info(f"â±ï¸  Duration: {duration:.1f} seconds")
    logger.info(f"ðŸŽ¯ Targets Scanned: {report['targets_scanned']}")
    logger.info(f"ðŸ› Total Findings: {report['total_findings']}")
    logger.info(f"ðŸ†• New Vulnerabilities: {report['unique_findings']}")
    logger.info(f"ðŸ’¾ Saved to Database: {saved_count}")
    logger.info("")
    logger.info("Severity Breakdown:")
    logger.info(f"   ðŸ”´ CRITICAL: {severity_counts['critical']}")
    logger.info(f"   ðŸŸ  HIGH: {severity_counts['high']}")
    logger.info(f"   ðŸŸ¡ MEDIUM: {severity_counts['medium']}")
    logger.info(f"   ðŸ”µ LOW: {severity_counts['low']}")
    logger.info(f"   âšª INFO: {severity_counts['info']}")
    if oob_interactions:
        logger.info(f"ðŸŒ OOB Interactions: {len(oob_interactions)}")
    logger.info("=" * 70)
    
    # Step 8: Send alerts for new findings
    if saved_count > 0:
        logger.info("ðŸ”” Phase 7: Sending alerts for new findings")
        await send_vulnerability_alerts_task(program_id, unique_findings)
    
    return report


@flow(name="continuous_vulnerability_scan")
async def continuous_vulnerability_scan(
    program_id: int,
    scan_interval_hours: int = 24,
) -> None:
    """
    Continuous vulnerability scanning.
    
    Runs vulnerability scans at regular intervals,
    automatically detecting and scanning new/changed assets.
    """
    logger.info(f"Starting continuous vulnerability scanning (every {scan_interval_hours}h)")
    
    while True:
        try:
            await vulnerability_scan_flow(program_id, force_rescan=False)
        except Exception as e:
            logger.error(f"Scan failed: {e}")
        
        # Wait for next scan
        import asyncio
        await asyncio.sleep(scan_interval_hours * 3600)
